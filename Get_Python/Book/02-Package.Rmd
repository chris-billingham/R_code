# Package

Part of the reason R has become so popular is the vast array of packages available at the **CRAN**.In the last few years, the number of packages has grown exponentially!

Untill Aug 2017, the CRAN package repository features 11288 available packages.

In order to use a package.we need to first download it and then load the package.

pip for conda
https://pypi.python.org/pypi/pip
https://conda.io/

Install a package from CRAN:
```{python eval=FALSE}
in cmd
conda install pandas
```

Check all installed package:
```{python eval=FALSE}
in cmd
conda list
```


load a package:
```{python}
import sys
```

Package on CRAN are been verified and tested by CRAN Team.
Be careful when you need to download Package from other sources.


Install a package from Github**(NOT recommended)**:


Get help on Package: question mark follow by package name


Uninstall packages:


**Popluar Package**:

**NumPy**:It provides an abundance of useful features for operations on n-arrays and matrices in Python. The library provides vectorization of mathematical operations on the NumPy array type.

**SciPy **: SciPy contains modules for linear algebra, optimization, integration, and statistics. The main functionality of SciPy library is built upon NumPy.

**Pandas **:Pandas is a Python package designed to do work with “labeled” and “relational” data simple and intuitive. Pandas is a perfect tool for data wrangling. It designed for quick and easy data manipulation, aggregation, and visualization.

**Matplotlib **:tailored for the generation of simple and powerful visualizations with ease is Matplotlib.

**Seaborn **:Seaborn is mostly focused on the visualization of statistical models; such visualizations include heat maps, those that summarize the data but still depict the overall distributions. Seaborn is based on Matplotlib.

**Bokeh**:Another great visualization library is Bokeh, which is aimed at interactive visualizations.


**Plotly**:Plotly is  a web-based toolbox for building visualizations.


**SciKit-Learn**:adding a set of algorithms for common machine learning and data mining tasks, including clustering, regression, and classification. 


**TensorFlow **: another high-profile entrant into machine learning, developed by Google as an open-source successor to DistBelief, their previous framework for training neural networks. TensorFlow uses a system of multi-layered nodes that allow you to quickly set up, train, and deploy artificial neural networks with large datasets.

**Scrapy**:is an aptly named library for creating spider bots to systematically crawl the web


**NLTK**:is a set of libraries designed for Natural Language Processing (NLP). NLTK’s basic functions allow you to tag text, identify named entities, and display parse trees, which are like sentence diagrams that reveal parts of speech and dependencies. 











