# Data Modeling 

## statistical inference
```{r echo=FALSE, out.width='50%'}
knitr::include_graphics('./Pic/inference001.png')
```
### sampling

### inference

## statistic distribution 

## statistic Test

## Modeling


### supervised learning
Supervised learning is the machine learning task of inferring a function from labeled training data.[1] The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples.



#### Linear regression

Regression to the mean by Francis Galton

Regression to the mean is a statistical phenomenon that can make natural variation in repeated data look like real change. It happens when unusually large or small measurements tend to be followed by measurements that are closer to the mean.

"Regression to the mean is so powerful that once-in-a-generation talent basically never sires once-in-a-generation talent. It explains why Michael Jordanâ€™s sons were middling college basketball players and Jakob Dylan wrote two good songs. It is why there are no American parent-child pairs among Hall of Fame players in any major professional sports league."
by economist Seth Stephens-Davidowitz

linear regression is a linear approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X




##### Simple linear regression

One of the regression model,using one vairable to predict a continual target
```{r}
library(ggplot2)
# Scatterplot with regression line
ggplot(data = mtcars, aes(x = mpg, y = wt)) + 
  geom_point() + 
  geom_smooth(method = "lm")
```


Find the Equation : 
$$Y=\beta_0+\beta_1*X$$

Residuals:
$$e=Y-\hat{x}$$

So that minimize:
$$\sum_{i=1}^n e_i^2$$

Fit a model:
```{r}
mod=lm(wt ~ mpg, data = mtcars)# mpg to predict wt
mod
#coef(mod)
class(mod)
summary(mod)

head(fitted.values(mod))
```

Using broom package to get the predict value and residue :
```{r eval=FALSE}
#install.packages('broom')
library(broom)
head(augment(mod))

```

make a prediction
```{r eval=FALSE}
#install.packages('broom')
library(broom)
library(dplyr)
head(augment(mod)%>%select(wt,mpg,.fitted,.se.fit))

```


Outlier impact:leverage:the distance of that observation from the mean of the explanatory variable

```{r eval=FALSE}
#install.packages('broom')
library(broom)
library(dplyr)
augment(mod)%>%select(wt,mpg,.fitted,.hat)%>%arrange(desc(.hat))%>%head()
```


Outlier impact:Influence:using Cook's distance, which incorporates both the leverage and residual of each observation.
```{r eval=FALSE}
#install.packages('broom')
library(broom)
library(dplyr)
augment(mod)%>%select(wt,mpg,.fitted,.cooksd)%>%arrange(desc(.cooksd))%>%head()
```





##### Multi linear regression
One of the regression model,using Multi vairables to predict a continual target

caret package




#### Logistic regression
classification model




#### Random forest
classification model
#### Support Vector Machine

#### Association rule learning
classification model
#### Neural Network


### unsupervised learning
Unsupervised machine learning is the machine learning task of inferring a function to describe hidden structure from "unlabeled" data (a classification or categorization is not included in the observations).

#### K mean clusting

#### hierarchical clustering



## Parallel computing

