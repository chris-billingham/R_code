# Appendix


## The Giants
"If I have seen further, it is by standing on the shoulders of giants" by Isaac Newton.

Hadley Wickham

Joe Cheng

Yihui Xie 


## The Talk

Expressing yourself in R by  Hadley Wickham on 2014 @Stanford Seminar
https://www.youtube.com/watch?v=wki0BqlztCo



All Things R and RStudio
Q&A with J.J. Allaire, Hadley Wickham & Joe Cheng
Moderator: Joseph Rickert
https://www.rstudio.com/resources/videos/all-things-r-and-rstudio/


Finding and Telling Stories with R
by Andrew Flowers from FiveThirtyEight
https://www.rstudio.com/resources/videos/finding-and-telling-stories-with-r/

install.packages('fivethirtyeight')
library(fivethirtyeight)

1.Novelty
2.Outlier
3.Archetype
4.Trend
5.Debunking
6.Forecast


## The Walk

follow the master,work with the master and become the master 


## Learning resource


## Reference

### web 
www.r-bloggers.com

www.rstudio.com

www.r-project.org

www.wikipedia.org

https://www.youtube.com/user/marinstatlectures/videos




### book
1.The Lady Tasting Tea by David Salsburg

2.The Art of R Programming by Norman Matloff 

3.R for Data Science by Hadley Wickham and Garrett Grolemund

4.Introduction to Statistical Learning(ISLR) by Gareth James,Daniela Witten and Trevor Hastie Robert Tibshirani




Learn R, in R.  http://swirlstats.com/
```{r eval=FALSE}
install.packages("swirl")
library("swirl")
swirl()
```

www.datacamp.com

## Appendix to the book To Chapter 4 :qplot()[quickly plotting]


### Scatter plot:
```{r message=FALSE,warning=FALSE}
library(ggplot2)
#Scatter
qplot(displ,hwy,data=mpg)
qplot(displ,hwy,data=mpg,color=drv)# group with color
qplot(displ,hwy,data=mpg,shape=drv)# group with shape
```

### Scatter and line:
```{r warning=FALSE,message=FALSE}
library(ggplot2)
qplot(displ,hwy,data=mpg,geom=c('point','smooth'))
```

#### Histogram:
```{r message=FALSE,warning=FALSE}
qplot(hwy,data=mpg)
qplot(hwy,data=mpg,fill=drv)# group with color
```


### facets
```{r message=FALSE,warning=FALSE}
qplot(displ,hwy,data=mpg,facets = .~drv)#facet
qplot(hwy,data=mpg,facets = .~drv,binwidth=2)# facet and binwidth setting
```


## Appendix to the book To Chapter 3 :Web Data and Big Data
finding the internet

### import Data from URL
```{r eval=FALSE}
# Direct import 
csv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1561/datasets/chickwts.csv"
csv_data <- read.csv(file = csv_url)

# Download the file with download.file()
csv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1561/datasets/chickwts.csv"
download.file(url = csv_url, destfile = "feed_data.csv")
# Read it in with read.csv()
csv_data <- read.csv(file = "feed_data.csv")
```

Save data to R-specific file format
```{r eval=FALSE}
# Save it to disk with saveRDS()
saveRDS(object = csv_data, file = "modified_feed_data.RDS")

# Read it back in with readRDS()
modified_feed_data <- readRDS(file = "modified_feed_data.RDS")
```

### API

#### API package
wiki API example: 
```{r eval=FALSE}
# Load pageviews
#install.packages('pageviews')
library(pageviews)

# Get the pageviews for "Hadley Wickham"
hadley_pageviews <- article_pageviews(article = "Hadley Wickham")

# Examine the resulting object
str(hadley_pageviews)
hadley_pageviews
```




Twitter API example: 

#### GET and POST request

GET request:
```{r eval=FALSE}
library(httr)
# Make a GET request to http://httpbin.org/get
get_result <- GET(url = "http://httpbin.org/get")

# Print it to inspect it
get_result
```

POST request:
```{r eval=FALSE}
# Load the httr package
library(httr)

# Make a POST request to http://httpbin.org/post with the body "this is a test"
post_result <- POST(url = "http://httpbin.org/post", body = "this is a test")

# Print it to inspect it
post_result
```

Get request content:
```{r eval=FALSE}
# Make a GET request to url and save the results
pageview_response <- GET(url)

# Call content() to retrieve the data the server sent back
pageview_data <- content(pageview_response)

# Examine the results with str()
str(pageview_data)
```

directory-based URL:
```{r eval=FALSE}
# Construct a directory-based API URL to `http://swapi.co/api/`,
# looking for person `1` in `people`
directory_url <- paste("http://swapi.co/api", "people", "1", sep = "/")

# Make a GET call with it
result <- GET(directory_url)
```

parameter-based URL:
```{r eval=FALSE}
# Create list with nationality and country elements
query_params <- list(nationality = "americans", 
    country = "antigua")
    
# Make parameter-based call to httpbin, with query_params
parameter_response <- GET("https://httpbin.org/get", query = query_params)

# Print parameter_response
parameter_response
```


### JSON
Object type:{"first_name": "Jason","last_name": "Bourne"}
arrary type:[123,4345,56]

```{r eval=FALSE}
library(httr)
library(jsonlite)
# Make a GET request to http://httpbin.org/get
get_result <- GET(url = "http://httpbin.org/get")

# Print it to inspect it
get_result
# check tpye
http_type(get_result)
# Examine returned text with content()
content(get_result, as = "text")
# Parse response with content()
content(get_result, as = "parsed")
# Parse returned text with fromJSON()
fromJSON(content(get_result, as = "text"))
```

### XML 
```{r eval=FALSE}
library(xml2)
x <- read_xml("<foo>
              <bar>text <baz id = 'a' /></bar>
              <bar>2</bar>
              <baz id = 'b' />
              </foo>")

xml_name(x)
xml_children(x)
# Find all baz nodes anywhere in the document
baz <- xml_find_all(x, ".//baz")
baz

xml_path(baz)

xml_attr(baz, "id")
```

### XPATH
```{r eval=FALSE}
library(xml2)
x <- read_xml("<foo>
              <bar>text <baz id = 'a' /></bar>
              <bar>2</bar>
              <baz id = 'b' />
              <bar type='123'>2</bar>  
              </foo>")

node=xml_find_all(x, "//bar")
# xml node collection
node
# get text
xml_text(node)
# find attributes 'type'
xml_attr(node, "type")


```


### Web Crawler

rvest by xpath:
```{r eval=FALSE}
# Load rvest
library(rvest)

# Hadley Wickham's Wikipedia page
test_url <- "https://en.wikipedia.org/wiki/Hadley_Wickham"

# Read the URL stored as "test_url" with read_html()
test_xml <- read_html(test_url)

# Print test_xml
test_xml

# xpath 
test_node_xpath='//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"vcard\", \" \" ))]'

# Use html_node() to grab the node with the XPATH stored as `test_node_xpath`
node <- html_node(x = test_xml, xpath = test_node_xpath)

# Print the first element of the result
node[[1]]
```


Read table:
```{r eval=FALSE}
url='https://www.basketball-reference.com/leagues/NBA_2017.html'
xml <- read_html(url)
all_table <- html_table(xml)
all_table
all_table[[1]] # first table

# find table with table name(xpath)
node <- html_node(x = xml, xpath = '//*[@id="confs_standings_E"]')
table <- html_table(node)
table
```
rvest by CSS:

Example:
```{r eval=FALSE}
# Load httr
library(httr)
# The API url
base_url <- "https://en.wikipedia.org/w/api.php"
# Set query parameters
query_params <- list(action = "parse", 
  page = "Hadley Wickham", 
  format = "xml")
# Get data from API
resp <- GET(url = base_url, query = query_params)
# Parse response
resp_xml <- content(resp)
```


### Big Data
R lets you write data analysis code quickly. With a bit of care, you can also make your code easy to read, which means that you can easily maintain your code too. In many cases, R is also fast enough at running your code.

Unfortunately, R requires that all your data be analyzed in memory (RAM), on a single machine. This limits how much data you can analyze using R. There are a few solutions to this problem, including using Spark.

#### Spark
Spark is an open source cluster computing platform. That means that you can spread your data and your computations across multiple machines, effectively letting you analyze an unlimited amount of data. The two technologies complement each other strongly. By using R and Spark together you can write code fast and run code fast!

sparklyr is an R package that lets you write R code to work with data in a Spark cluster. It has a dplyr interface, which means that you can write (more or less) the same dplyr-style R code, whether you are working with data on your machine or on a Spark cluster.

```{r eval=FALSE}
# Load sparklyr
library(sparklyr)
#spark_install()
# Connect to your Spark cluster
spark_conn <- spark_connect("local")
# Print the version of Spark
spark_version(spark_conn)
# Disconnect from Spark
spark_disconnect(spark_conn)
```

```{r eval=FALSE}

# Load dplyr
library(dplyr)
# Explore track_metadata structure
str(track_metadata)
# Connect to your Spark cluster
spark_conn <- spark_connect("local")
# Copy track_metadata to Spark
track_metadata_tbl <- copy_to(spark_conn, track_metadata)
# List the data frames available in Spark
src_tbls(spark_conn)
# Link to the track_metadata table in Spark
track_metadata_tbl <- tbl(spark_conn, "track_metadata")
# See how big the dataset is
dim(track_metadata_tbl)
# See how small the tibble is
object_size(track_metadata_tbl)
# Disconnect from Spark
spark_disconnect(spark_conn)


```
#### Hadoop


## Appendix to the book To Chapter 6 : Parallel computing